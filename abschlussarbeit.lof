\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1}{\ignorespaces A short characterization of the most important features and drawbacks of the generative models discussed in this section.~\citep {xiao2022tackling}}}{2}{}%
\contentsline {figure}{\numberline {2}{\ignorespaces Autoencoder: The encoder reduces the input dimension to a latent vector that captures the most important features. The decoder then uses this vector to reconstruct the input, with training aimed at minimizing reconstruction loss. TODO\spacefactor \@m {} ADD LOSS\spacefactor \@m {} https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2}}{4}{}%
\contentsline {figure}{\numberline {3}{\ignorespaces Functionality of a Variational Autoencoder, demonstrating incorporation of the latent distribution~--~the mean and standard deviation~--~for enhancing latent space regularization.}}{5}{}%
\contentsline {figure}{\numberline {4}{\ignorespaces Simplified functionality of a Generative Adversarial Network}}{6}{}%
\contentsline {figure}{\numberline {5}{\ignorespaces Forward process adding noise to an image}}{8}{}%
\contentsline {figure}{\numberline {6}{\ignorespaces Forward process adding noise to an image}}{9}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {7}{\ignorespaces Neuronal Radiance Field. Figure taken From Mildenhall~\cite {mildenhallNERF}~--~Figure 2}}{12}{}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
