\subsection{Fantasia3D}\label{fantasia3D}

The model proposed by \citeauthor{chen2023fantasia3d} takes a different approach to generating 3D models from text input, in particular by disentangling geometry and appearance in the generated 3D models.
This method offers a more detailed rendering quality compared to conventional Neural Radiance Fields (NeRFs), which use volume rendering to combine the learning of surface geometry with pixel colors. This conventoional approach limits effective surface recovery, lacking the capability to track the surface of an object and tune detailed material and texture. In contrast, Fantasia3D achieves more realistic outputs with its hybrid scene representation of DMTet, ``which maintains a deformable tetrahedral grid and a differentiable mesh extraction layer; deformation can thus be learned through the layer to explicitly control the shape generation'' \citep{chen2023fantasia3d}

\begin{figure}[ht]
  \centering
    \includegraphics[width=1\columnwidth]{figures/Fantasia3D.png}
    \caption{Outline of the method Fantasia3D. Figure taken From \citep{chen2023fantasia3d}}\label{fig:figureFantasia}
\end{figure}

In the geometry stage, Fantasia3D relies on a Deformable Mesh Tetrahedralization (DMTet), which parametrizes the 3D geometry as a Multi-Layer Perceptron (MLP). Initially, Fantasia3D renders and encodes surface normals and object masks extracted from DMTet. However, in later stages, the model refines its approach by using only the rendered normal map for shape encoding \citep{chen2023fantasia3d}. The default initialization of DMTet is an ellipsoid, but the model also accepts custom inputs, providing flexibility in shape generation.

Appearance Modeling involves training another MLP \( \Gamma \), which is responsible for applying the Bidirectional Reflectance Distribution Function (BRDF) to a pre-learned DMTet. This function is crucial for ``predict[ing] parameters of surface material and supports high-quality 3D generation via photorealistic rendering'' \citep{chen2023fantasia3d}. The BRDF focuses on the following parameters to achieve accurate shading of the geometry: the diffuse value \(k_d\), the combined roughness and metallic properties \(k_{rm}\), and the normal variation \(k_n\). These are predicted using the formula \((k_d, k_{rm}, k_n) = \Gamma(\beta(p); \gamma)\) by \citeauthor{chen2023fantasia3d}, where \(\beta(p)\) represents the surface properties at point \(p\) and \(\gamma\) denotes network parameters.

Shooting rays for each pixel into the scene and rendering the color of each ray produces an image, which is shown by the formula provided by \citeauthor{chen2023fantasia3d}:

\[ L(p, \omega) = \int_{\Omega} L_i(p, \omega_i) f(p, \omega_i, \omega) (\omega_i \cdot n_p) d\omega_i \] \citep{chen2023fantasia3d},

where \( L(p, \omega) \) is the light at point \( p \) in direction \( \omega \), \( \Omega \) is the hemisphere around \( p \), \( L_i \) is the incoming light, \( f \) is the BRDF, and \( n_p \) is the normal at \( p \). This equation fundamentally captures how light interacts with the surface at every point. The model also separates the rendering into diffuse and specular components. 

The learning process for both the geometry and appearance aspects of Fantasia3D employs Multi-Layer Perceptrons (MLPs), which are refined through a stable diffusion model previously trained, and utilizes SDS loss for optimization \citep{rombachStableDiffusion}. 

Fantasia3D offers a high degree of user interactivity, permitting the incorporation of both custom and predefined generic 3D shapes, thus greatly enhancing the versatility and user engagement in the content creation process. The separation of geometry and appearance generation also ensures compatibility with widely-used graphics engines \citep{chen2023fantasia3d}. Despite its capabilities in creating high-quality 3D models from textual descriptions, Fantasia3D encounters specific challenges. One notable limitation is its struggle with accurately generating complex geometries like hair, fur, and grass \citep{chen2023fantasia3d}. Furthermore, the model is currently not able to generate complete scenes as focus is currently lying on individual object generation \citep{chen2023fantasia3d}.