\section{Variational Autoencoders - VAEs}
\label{VAEs}

Generative models, as defined in the seminal work by \citeauthor{diggleImplicitPrescribed}, fall into two primary categories: prescribed and implicit models. Prescribed models leverage well-formulated, often parametric, mathematical expressions for the probability density function (pdf), thus facilitating easier analytical interpretation of the distributions. Implicit models, on the other hand, synthesize new data samples without an explicit pdf, approximating the data distribution based on the training dataset \citep{diggleImplicitPrescribed}. Variational Autoencoders (VAEs) are aligned with the prescribed models category, given their reliance on an explicit formulation of the pdf to operate efficiently. This feature makes VAEs suitable for tasks that require not only the generation but also the understanding of complex data distributions \citep{kingmaVAE,rezendeVAE,GoodfellowDeepLearning}. In contrast, Generative Adversarial Networks (GANs) are a prime example of the implicit models category \citep{goodfellowGAN}.

VAEs are essentially based on the architecture of Autoencoders, which apply an iterative process to identify the optimal encoder and decoder pair, aiming to minimize the reconstruction loss while preserving substantial information after dimensionality reduction. The encoder compresses the input data into a low-dimensional representation, or "code" \citep{hintonCode, GoodfellowDeepLearning}, which captures the most relevant features of the input, within a latent space. The decoder then reconstructs the original input from this compressed latent vector, with the error between the output and original data being backpropagated to optimize the modelâ€™s weights \citep{GoodfellowDeepLearning, michelucci2022introduction}. Although this process balances data compression and information preservation, a drawback remains that if the encoder and decoder are too closely matched, the latent space becomes uncontrollable resulting in a lack of interpretability and regularity \citep{michelucci2022introduction}.

Autoencoders, in their essence, aim for approximate rather than perfect replication of the input data, which forces the model to prioritize certain aspects of the input to copy, often learning useful properties of the data \citep{GoodfellowDeepLearning}. This dimensionality reduction proves beneficial in enhancing classification tasks' efficiency by reducing computational costs and memory overhead, and improving information retrieval in low-dimensional spaces \citep{GoodfellowDeepLearning}. However, traditional autoencoders fall short in generating new data due to the unregulated nature of the latent spaces.

The problem with unregulated latent spaces in autoencoders becomes obvious when it comes to generating new data. Randomly selecting a point from this latent space and trying to decode it leads to unusable results in most cases. Simply trying to organize the latent space proves difficult as its structure is affected by the distribution of the source space, the dimensionality of the latent space, and the architecture of the encoder \citep{michelucci2022introduction}.
In essence, traditional autoencoders are not designed to generate new data; their main function is to copy and reconstruct the given input \citep{GoodfellowDeepLearning}.

\begin{figure}[ht]
    \centering
      \hspace{.8cm}
      \includegraphics[width=.7\columnwidth]{figures/Autoencoder.png}
      \caption{Autoencoder: The encoder reduces the input dimension to a latent vector that captures the most important features. The decoder then uses this vector to reconstruct the input, with training aimed at minimizing reconstruction loss.}
      \label{fig:figureAE}
\end{figure}

VAEs tackle the limitations of traditional autoencoders by implementing enhanced regularization of the latent space during training. Although the iterative process largely remains the same, VAEs deviate by encoding a distribution over the latent space instead of a single point from the input. Specifically, the encoder outputs parameters \(\mu\) and \(\sigma\) which define a Gaussian distribution \(N(\mu, \sigma^2)\) \citep{doerschVAE}. from which the latent variable \(z\) is sampled. This sampled point is utilized to calculate the reconstruction error, which is then backpropagated to update the model's weights.

A pivotal objective of VAEs is to compute the data likelihood, \(P(X)\), through strategic sampling of latent variables \(z\) \citep{doerschVAE}. Employing a function \(Q(z|X)\) to provide a distribution over latent variables \(z\) that are most likely to generate a given data point \(X\), VAEs concentrate computation on the values of \(z\) that significantly contribute to \(P(X)\) \citep{doerschVAE}. The core equation for VAEs is expressed as:

\[
\log P(X) - D_{KL} \left[ Q(z|X) \parallel P(z|X) \right] = \mathbb{E}_{z \sim Q} \left[ \log P(X|z) \right] - D_{KL} \left[ Q(z|X) \parallel P(z) \right]
\]
\\

\citep{doerschVAE}.
This equation sets the stage for how Variational Autoencoders work. First, it aims to  maximize the log likelihood of the data, denoted as \(\log P(X)\) \citep{doerschVAE}. In simpler words, a higher log likelihood means that the model is good at creating data that looks like the real data. The KL divergence term \(D_{KL} \left[ Q(z|X) \parallel P(z|X) \right]\) acts as an error term, ensuring that \(Q(z|X)\) accurately represents the posterior distribution \(P(z|X)\) \citep{doerschVAE}. This term measures how different the variational distribution \(Q(z|X)\), which is created by the encoder, is from the true posterior distribution \(P(z|X)\). By minimizing this divergence, it is ensured that \(Q(z|X)\) closely resembles \(P(z|X)\), which helps the encoder to capture the data's structure in a more compact, lower-dimensional space. On the right hand side, the first part determines the expected log-likelihood of the reconstruction. This part focuses on how well the model can recover the original data from the hidden or latent variables. The higher the value, the better it matches the original input. The KL divergence between the encoder and the prior acts as a regulatory mechanism that evaluates the deviation of the encoder's latent variable distribution \(z\) from a predefined ideal distribution \( P(z)\). This term guides the coder to generate latent variables that follow the desired distribution, which allows for a smoother learning and generalization process.

\begin{figure}[ht]
    \centering
      \hspace{.8cm}
      \includegraphics[width=.9\columnwidth]{figures/VAE.png}
      \caption{Functionality of a Variational Autoencoder, demonstrating incorporation of the latent distribution - the mean and standard deviation - for enhancing latent space regularization.}
      \label{fig:figureVAE}
\end{figure}

Despite their capabilities, VAEs exhibit some limitations. According to \citeauthor{GoodfellowDeepLearning}, the generated samples can often be blurry. The reason for this is not fully 
understood, but the blurriness observed may be due to their optimization process, which minimizes Kullback-Leibler divergence. This could lead the model to assign high probabilities to "points that occur in the training set, but may also assign high probability to other points [...]which may include blurry images" \citep{GoodfellowDeepLearning}. The Gaussian distribution often used in VAEs for the generative model may also contribute to this effect, as it can ignore minor features in the input data \citep{GoodfellowDeepLearning}. Another issue is that VAEs typically utilize only a small portion of the latent space, which might further compromise the quality of generated images \citep{GoodfellowDeepLearning}. The performance of the model is also sensitive to the choice of priors for the latent space, making hyperparameter tuning an essential aspect of working with VAEs \citep{kingmaVAE, higginsVAE}. 

