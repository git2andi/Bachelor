\section{Diffusion models}
\label{diffusion Models}

The limitations of GANs, which were just stated above, have led to the emergence of diffusion models, a class of AI tools that offer distinct advantages over traditional generative models. These models differ from GANs in their approach to modeling and generating data.  Instead of explicitly modeling the joint distribution of data and noise variables, diffusion models operate by progressively perturbing data with noise and then learning to reverse this process to generate new samples.

\citeauthor{yangdiffusionSummary} distingueshes between three main approaches that dominate the study of diffusion models, which are going to be discussed shortly: Denoising Diffusion Probabilistic Models (DDPMs) \citep{hoDDPMs,sohlDDPM}, Score-based Generative Models (SGMs) \citep{song2019SGM}, and Stochastic Differential Equations (Score SDEs) \citep{song2020score, song2021maximum}.

\subsection{Denoising Diffusion Probabilistic Models}
DDPMs employ two Markov chains, a forward chain and a reverse chain, also known as the forward and reverse diffusion processes \citep{sohlDDPM}. 

The forward diffusion process is similar to latent variable models, and it shares some characteristics with Variational Autoencoders, especially focusing on a hidden or latent feature space from the initial data distribution. However, the forward process in DDPMs is distinct because it follows a fixed path, called a Markov chain, that steadily adds Gaussian noise to the data following a certain pattern of variance, denoted as \( \beta_1, ..., \beta_T \) \citep{hoDDPMs}. This iterative process continues to add noise "until all structures [in the data] are lost" \citep{yangdiffusionSummary}. The introduction of noise aims to gradually move the data distribution towards a more manageable prior distribution \citep{yangdiffusionSummary, pooleDreamfusion}.

After taking a data point \( x_0 \) from the real data distribution \( q(x) \) (\( x_0 \sim q(x) \)), the forward diffusion process starts by adding noise. Specifically, in each step of the Markov chain, Gaussian noise with a variance \( \beta_t \) is added to \( x_{t-1} \), creating a new latent variable \( x_t \) with a distribution \( q(x_t | x_{t-1}) \). This is represented by the formula: 

\[ q(x_t | x_{t-1}) = \mathcal{N}(x_t; \mu_t = \sqrt{1 - \beta_t}x_{t-1}, \Sigma_t = \beta_t I) \] 

There, the identity matrix \( I \) shows that each dimension has the same standard deviation \( \beta_t \), highlighting the multi-dimensional aspect of the scenario. This process makes it possible to move smoothly from the input data \( x_0 \) to \( x_T \) in a structured manner, which is represented mathematically as the posterior probability: \[ q(x_{1:T} | x_0) = \prod_{t=1}^T q(x_t | x_{t-1}) \].

\begin{figure}[ht]
\centering
  \includegraphics[width=1\columnwidth]{figures/manta_DDMP3.png}
  \caption{Forward process adding noise to an image}
  \label{fig:figureDDPM}
\end{figure}

The reverse chain, in turn, uses an adaptive parameterized deep neural network to progressively remove the noise added by the forward chain. This iterative process aims to generate data patterns that closely resemble the original data by gradually reducing the noise. Training the reverse chain involves minimizing the Kullback-Leibler (KL) divergence between the joint distributions of the forward and reverse chains and maximizing the variational lower bound (VLB) of the log-likelihood of the data. The KL divergence measures the dissimilarity between two probability distributions, in this case, the forward and reverse chains. By minimizing the KL divergence, the reverse chain is trained to approximate the inverse of the forward process, effectively removing the noise added by the forward chain \citep{sohlDDPM}. The VLB provides a lower bound approximation of the log-likelihood, a measure of how well the model captures the underlying data distribution. Maximizing the VLB ensures that the reverse chain generates data patterns that closely match the original data distribution. \citep{hoDDPMs, sohlDDPM}. It's important to clarify that in the context of diffusion models, VLB is a term associated with variational inference, and its usage might not be directly equivalent to VAEs. Regarding what the reverse network predicts, it typically estimates the parameters of a conditional distribution that represents how to transform the noisy data at each step to recover the original data. This prediction guides the process of removing the noise introduced during the forward chain.

The incremental introduction of the forward and backward diffusion processes offers an advantage as "estimating small perturbations is more tractable than explicitly describing the full distribution with a single, non-analytically-normalizable, potential function" \citep{sohlDDPM}.
According to \citep{hoDDPMs}, the neural network in the reverse process can be trained to predict one of three possibilities: the mean value of the noise at each time step, the original image itself, or the noise of the image \citep{hoDDPMs}. As previously mentioned, the second approach is not as advantageous. Hence, the research focuses on the first and last possibilities, which are essentially identical but parameterized differently. Predicting the image noise allows for straightforward subtraction of the noise from the image, resulting in a less noisy version. By employing this method iteratively, it becomes possible to completely learn an image from noise.


\subsection{Score-Based Generative Models}
SGMs are a class of diffusion models that have gained prominence in the field of generative modeling due to their ability to capture complex data distributions and generate diverse and realistic samples.

Score-based generative models (SGMs) take a unique approach to generative modeling by prioritizing the learning of a score function that plays a central role in guiding the generative process. This score function aims to capture the Stein score \citep{steinScore}, which is essentially "the gradient of the log-density function at the input data point" \citep{song2019SGM}. To put it simply, the score can be seen as a vector field, indicating the direction in which the logarithm of the data density experiences the most significant growth \citep{song2019SGM}. In essence, the score function reveals how the data distribution responds to small variations within the data itself, serving as a guiding principle for the generative model. To train a neural network for SGMs, \cite{song2019SGM} employ a technique called score matching \citep{hyvarinenScoreMatching}. This involves estimating the score function for data points intentionally perturbed with Gaussian noise. This means that score matching effectively learns how to denoise the noisy data and restore the original data distribution. \citep{song2020improved}.

In order to generate Samples, Langevin Dynamics \citep{robertsLangevin} is used, which simulates a particle's movement in a field of potential energy, where the score function serves as the guiding force. By moving data samples along the direction of the score function, Langevin Dynamics effectively 'pulls' them toward areas where data density is higher, essentially places where they blend better with the overall dataset.

However, in the specific context of score-based generative modeling, there's a notable challenge to overcome. The estimated score function may not be entirely accurate in regions of the data space where there's minimal or no training data available \citep{song2019SGM}. In such cases, Langevin Dynamics might not converge correctly, resulting in complications during the sample generation process. 

To address this issue, \cite{song2019SGM} suggest "to perturb the data with random Gaussian noise of various magnitudes", while simultaneously estimate the score functions for these noise-altered data distributions. This approach ensures that the resulting data distribution doesn't condense into a lower-dimensional structure \citep{song2019SGM}.






\subsection{Stochastic Differential Equations}
\textcolor{blue}{Score SDEs provide a flexible framework for modeling and generating complex data distributions with inherent stochasticity. Unlike explicit modeling of the joint distribution of data and noise variables, SDEs focus on describing the dynamics of a diffusion process through differential equations. These equations incorporate both deterministic components that govern the overall trend of the process and stochastic components that account for the inherent randomness in the data generation process.
In Score SDEs, the diffusion process is represented as the continuous-time evolution of a random variable. By specifying the drift and diffusion coefficients within the SDEs, one can capture the behavior and evolution of the data distribution over time. The stochastic nature of SDEs allows for modeling intricate dependencies and capturing complex patterns in the data.
Training SDEs involves estimating the parameters of the drift and diffusion coefficients. This is typically done by maximizing the likelihood of the observed data under the SDE framework. Various estimation techniques, such as maximum likelihood estimation or Bayesian inference, can be employed to optimize the parameters.
SDEs can capture long-term dependencies and accurately model the evolution of data over time. Additionally, the stochastic nature of SDEs enables them to generate diverse and realistic samples by incorporating inherent randomness into the data generation process.}








DDPMs utilize two Markov chains - a forward chain and a reverse chain, known as forward and reverse diffusion processes, to introduce noise to data through a series of steps, following a normal distribution. This noise addition continues iteratively until the data evolves into pure isotropic Gaussian noise, steering the data distribution towards a more manageable prior distribution.

Now, delving deeper into the forward diffusion aspect, it's akin to latent variable models, sharing some similarities with Variational Autoencoders (VAEs) in that it refers to a concealed continuous feature space. Unlike flow-based models, which are bound to a specific type of neural network, the formulation of DDPMs embraces a more lenient structure through a Markov chain of TT steps. Each step in this chain relies solely on the preceding step, which is a mild assumption, thus offering a bit more flexibility.

When you have a data point \( x_0 \) sampled from the real data distribution \( q(x) \) (\( x_0 \sim q(x) \)), the forward diffusion process kicks in by adding noise. Specifically, at each step in the Markov chain, Gaussian noise with a variance \( \beta_t \) is added to \( x_{t-1} \), generating a new latent variable \( x_t \) with a distribution \( q(x_t | x_{t-1}) \). This can be expressed by the equation: \( q(x_t | x_{t-1}) = \mathcal{N}(x_t; \mu_t = (1 - \beta_t)x_{t-1}, \Sigma_t = \beta_t I) \). Here, the identity matrix \( I \) indicates that each dimension possesses the same standard deviation \( \beta_t \), as we are dealing with a multi-dimensional scenario.

The process facilitates a smooth transition from the input data \( x_0 \) to \( x_T \) in a closed form, mathematically represented as the posterior probability: \( q(x_{1:T} | x_0) = \prod_{t=1}^T q(x_t | x_{t-1}) \). The term \( :: \) in \( q(x_{1:T}) \) signifies the repeated application of \( q \) from timestep 1 to \( T \), termed as trajectory.

However, a hiccup arises when, for timestep \( t = 500 < T \), the model requires the application of \( q \) 500 times to sample \( x_t \). This is where the reparametrization trick shines by enabling tractable closed-form sampling at any timestep. Defining \( \alpha_t = 1 - \beta_t \), \( \overline{\alpha}_t = \prod_{s=0}^t \alpha_s \), and with \( \epsilon_{0,...,t-2,t-1} \sim \mathcal{N}(0,I) \), the trick recursively demonstrates that \( x_t = (1 - \beta_t) x_{t-1} + \beta_t \epsilon_{t-1} = \alpha_t x_{t-2} + (1 - \alpha_t) \epsilon_{t-2} = ... = \overline{\alpha}_t x_0 + (1 - \overline{\alpha}_t) \epsilon_0 \). Thus, to yield a sample \( x_t \), the distribution \( x_t \sim q(x_t | x_0) = \mathcal{N}(x_t; \overline{\alpha}_t x_0, (1 - \overline{\alpha}_t) I) \) is employed.

The variance parameter \( \beta_t \) can either remain constant or follow a schedule across the \( T \) timesteps. A variety of schedules like linear, quadratic, or cosine can be used. While the original DDPM authors used a linear schedule that increased from \( \beta_1 = 10^{-4} \) to \( \beta_T = 0.02 \), Nichol et al. 2021 found that a cosine schedule performed better. This variance scheduling and the reparametrization trick collectively enable sampling of the latent variable \( x_t \) at any arbitrary timestep, setting the stage for calculating a tractable objective loss \( L_t \) later on.