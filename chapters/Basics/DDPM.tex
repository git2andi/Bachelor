\subsection{Denoising Diffusion Probabilistic Models}\label{DDPMs}

Central to the concept of Denoising Diffusion Probabilistic Models (DDPMs) are two Markov chains: the forward chain and the reverse chain, also known as the forward and reverse diffusion processes \citep{sohlDDPM}.

The forward diffusion process, sharing some similarities with VAEs, focuses on a latent feature space of the initial data distribution. However, DDPMs differ in that the forward process in DDPMs ``is fixed to a Markov chain that gradually [over a span of T steps] adds Gaussian noise to the data according to a variance schedule \(\beta_1, \ldots, \beta_T \)''~\cite{hoDDPMs}. This process gradually perturbs the data's structure, eventually resulting in an image of pure noise, with the aim of gradually steering the data distribution towards a more manageable prior distribution \citep{yangdiffusionSummary, pooleDreamfusion}. The mathematical formulation of the forward process is given by \citeauthor{martinez2023understanding}: \[
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t}x_{t-1}, \beta_t I) \quad \text{where} \quad \sqrt{1 - \beta_t}x_{t-1} = \mu_t \quad \text{and} \quad \beta_t I = \Sigma_t
\] In this equation, the model first adjusts the previous data point \( x_{t-1} \) to get \( x_t \), the data point at the current step. This adjustment follows a Gaussian distribution and is done using the term \( \sqrt{1 - \beta_t} x_{t-1} \), which slightly reduces the intensity or strength of the previous data point \citep{sohlDDPM, hoDDPMs}. Noise is added to the data at each timestep \(t\) using a covariance matrix \(\beta_t I\), where \(I\) is the identity matrix. This ensures uniform and independent distribution of noise across all data elements. The process of gradually adding noise is crucial, as it helps the model learn the structure and characteristics of noise. This understanding is key to the reverse process in DDPMs, where the model uses its knowledge of how images degrade to effectively reverse the effects of the noise.

The process of adding noise over the entire sequence from the original data point \( x_0 \) to \( x_T \) is captured another formular by \citeauthor{martinez2023understanding}: \[q(x_{1:T} | x_0) = \prod_{t=1}^T q(x_t | x_{t-1}) \] Built upon the Markov property, the formula implies that each step depends solely on the previous step, allowing for a systematic and gradual transformation from \( x_0 \) to \( x_T \) \citep{martinez2023understanding}.

\begin{figure}[ht]
\centering
  \includegraphics[width=1\columnwidth]{figures/manta_DDMP3.png}
  \caption{Illustration of the Forward Diffusion Process in DDPMs: This figure demonstrates the gradual addition of Gaussian noise to an image over multiple steps. Each subsequent image from left to right shows an increased level of noise, culminating in the far-right image, which represents a state of pure noise.}\label{fig:figureForwardProcess}
\end{figure}

The reverse diffusion process employs a neural network parameterized by \(\Theta\), to approximate the inverse of the forward process. It estimates the prior state of data points, \( x_{t-1} \), from their current noisy state, \( x_t \), using the probability distribution function \( p_\theta(x_{t-1} | x_t) \), as given by \citeauthor{martinez2023understanding}. This process is modeled as a normal distribution where the mean \( \mu_\theta(x_t, t) \) and covariance \( \Sigma_\theta(x_t, t) \) are determined by the neural network \citep{yangdiffusionSummary}.\[
  p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
\] \[p_\theta(x_{0:T}) = p_\theta(x_{T}) \prod_{t=1}^T p_\theta(x_{t-1} | x_t) \] The latter function \(p_\theta(x_{0:T})\) is also taken from \citeauthor{martinez2023understanding} and captures the probability of the entire data sequence under the reverse process, beginning with an estimate of the final noisy data point \(p_\theta(x_{T})\) and progressively reconstructing the data by removing noise at each step \citep{hoDDPMs,martinez2023understanding}. The reverse process is also a Markov chain and involves the neural network learning to predict the reverse diffusion parameters \(\Theta\) at each timestep \citep{yangdiffusionSummary}. The aim is to generate samples that are statistically similar to the original training data in order to increase the probability that these new samples belong to the same data distribution \citep{yangdiffusionSummary}.

In the reverse process, the neural network can be trained to predict one of three possibilities: the mean of the noise at each time step, the original image itself, or the noise of the image \citep{hoDDPMs}. The second approach is not as advantageous as the ``estimating small perturbations is easier than explicitly describing the entire distribution with a single, non-analytically normalizable potential function'' \citep{sohlDDPM}. Focusing on the prediction of image noise is preferable because it allows a simple subtraction of the noise from the image, resulting in a less noisy version and thus also allowing an iterative generation of an image from the noise.

Despite their effectiveness, DDPMs are not without challenges. The most significant of these is the computational time required for generating new samples, which is due to ``a Markov process [that] has to be simulated at each generation step, which greatly slows down the process'' \citep{martinez2023understanding}.