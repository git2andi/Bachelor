\subsection{Score-Based Generative Models}\label{SGMs}

In the domain of generative modeling, Score-Based Generative Models (SGMs), as introduced by \citeauthor{song2019SGM}, distinguish themselves by prioritizing the learning of a score function using the Stein score \citep{steinScore}. The score is ``the gradient of the log-density function at the input data point'' \citep{song2019SGM}, serving as a guide towards higher data density regions. The score function is central in SGMs, akin to how DDPMs use a forward process of adding noise, but with a different purpose. In SGMs, the score guides the model to areas in the data space where the probability of finding similar data points to the training set is higher.

SGMs begin their learning process with a set of training data. Common data distributions indicate areas of high probability, while rare inputs usually result in low represented data regions. The role of SGMs is to learn how to work with that data space. In practice, the model \(s_\theta(x)\) is trained to replicate the score function \(\nabla_x \log{p(x)}\) of a data distribution \(p(x)\) using so called score matching \citep{hyvarinenScoreMatching}. Training of such models is achieved by ``minimizing the Fischer divergence between the model and the data distributions''~\citeauthor{song2021score}, denoted as: 

\[
\mathbb{E}_{p_{(x)}} \left[ \| \nabla_x \log{p(x)} - s_\theta(x) \| ^2_2 \right]
\] 

This divergence assesses how closely the model's score aligns with the actual data score by calculating their squared differences. The integration of score matching into this process means that no precise knowledge of the true data value is required \citep{song2021score}. The goal here is to align the model's predictions with the real data's log-likelihood gradient as closely as possible. Under specific conditions, this method can reliably ensure that the model's score accurately reflects the true score of the data distribution \citep{song2019SGM}.

In the generation of new samples, SGMs start with a random or noise image. Using Langevin dynamics, an iterative process described by \citep{robertsLangevin}, the model samples from a distribution \( p(x) \) using its score function. Following \citeauthor{song2021score}, it starts with a random prior distribution \( x_0 \sim \pi(x) \), and iteratively updates the chain by:

\[ 
  x_{i+1} \leftarrow x_i + \epsilon \nabla_x \log p(x) + \sqrt{2 \epsilon} z_i,
\]

where each iteration updates the sample \(x_i\) by a small step size \(\epsilon\) in the direction of the data's score function, guiding the sample towards the target data distribution. This process theoretically converges the distribution to \( p(x) \) as \( \epsilon \) becomes smaller and iterations increase \citep{song2019SGM,song2021score}.

A notable challenge in SGMs arises when estimating the score function in less represented data regions. Inaccuracies here may hinder the convergence of Langevin Dynamics \citep{song2019SGM}. To mitigate this, \citeauthor{song2019SGM} recommend introducing Gaussian noise to the data and estimating scores for these noise-altered distributions, an approach termed Noise Conditional Score Networks (NCSNs). This strategy prevents the data distribution from collapsing into a lower-dimensional structure, ensuring more robust sample generation \citep{song2019SGM}.

The core idea of Noise Conditional Score Networks is to generate realistic data samples by effectively navigating through a series of noise-perturbed data distributions that gradually converge to the true data distribution. This process is facilitated by introducing Gaussian noise \(\sigma\) into the data at multiple levels \citep{song2019SGM}. The NCSN, represented \(s_\theta(x, \sigma)\), is trained to estimate scores for these noise-perturbed distributions. This training allows the network to adjust its predictions based on different noise levels to ensure accurate and relevant point estimates. Generating new samples begins with a state of pure noise and uses annealed Langevin dynamics in order to gradually reduce the noise level \(\sigma\), ending when a sufficiently low noise level yields a sample that accurately represents the modeled data \citep{song2019SGM}.