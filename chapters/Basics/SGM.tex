\subsection{Score-Based Generative Models}\label{SGMs}

Score-Based Generative Models (SGMs), as introduced by \citeauthor{song2019SGM}, distinguish themselves by prioritizing the learning of a score function using the Stein score \citep{steinScore}. The score is ``the gradient of the log-density function at the input data point'' \citep{song2019SGM}. It serves as a guide towards regions with higher data density~-~where it's more likely to find data points similar to those in the training set.

SGMs start with learning on a set of training data. The role of SGMs is to learn how to work with that data space. In practice, the model \(s_\theta(x)\) is trained to replicate the score function \(\nabla_x \log{p(x)}\) of a data distribution \(p(x)\) using so called score matching \citep{hyvarinenScoreMatching}. Training of such models is achieved by ``minimizing the Fischer divergence between the model and the data distributions''~\citeauthor{song2021score}, denoted as: \[
\mathbb{E}_{p_{(x)}} \left[ \| \nabla_x \log{p(x)} - s_\theta(x) \| ^2_2 \right]
\] This divergence assesses how closely the model's score aligns with the actual data score by calculating their squared differences. The integration of score matching into this process means that no precise knowledge of the true data value is required \citep{song2021score}. The goal here is to align the model's predictions with the real data's log-likelihood gradient as closely as possible.

When it comes to generating new data, SGMs start with a random or noise image and employ a method called Langevin dynamics, an iterative process described by \citep{robertsLangevin}. In this iterative process, as shown in the following equation by \citeauthor{song2021score}, the sample \(x_i\) is updated in small steps, using the score function of the data as a guide. With each step, the sample is brought a little closer to the target data distribution. Theoretically, the sample distribution converges to the target distribution \(p(x)\) when the step size \(\epsilon\) becomes smaller and the number of iterations increases \citep{song2019SGM,song2021score}.\[ 
  x_{i+1} \leftarrow x_i + \epsilon \nabla_x \log p(x) + \sqrt{2 \epsilon} z_i,
\]

One of the challenges in SGMs is estimating the score function in data regions that are not well represented, which can affect the efficiency of the Langevin dynamics process \citep{song2019SGM}. To solve this problem, \citeauthor{song2019SGM} introduces a technique known as Noise Conditional Score Networks (NCSNs). Here, Gaussian noise is applied to the data at different levels and the network is trained to estimate the scores for these noise-altered distributions. The network learns to adjust its predictions based on the noise level, resulting in more accurate sample generation \citep{song2019SGM}.

The core idea of Noise Conditional Score Networks \(s_\theta(x, \sigma)\) is to generate realistic data samples by effectively navigating through a series of noisy data distributions that gradually converge to the true data distribution. The generation of new samples starts with a state of pure noise and uses annealed Langevin dynamics to gradually reduce the noise level \(\sigma\) and ends when a sufficiently low noise level yields a sample that accurately represents the modeled data \citep{song2019SGM}.
