\section{Multilayer Perceptron~--~MLP}\label{MLP}

Multilayer Perceptrons (MLPs) form a fundamental class of artificial neural networks in machine learning, characterized by their layered structure, including input, hidden, and output layers \citep{noriega2005multilayer, MurtaghMLP}.  Central to their function is the activation function, typically the Logistic Sigmoid Function in MLPs, which transforms the weighted inputs into node outputs in a continuous and differentiable manner, facilitating gradient-based optimization \citep{MurtaghMLP, noriega2005multilayer}. The learning process in MLPs is supervised, involving initial random weight assignment, followed by training through pattern presentation, output comparison, and backward error propagation, typically using gradient descent methods to minimize errors \citep{noriega2005multilayer}. This process is guided by the generalized delta rule or backpropagation, which adjusts network weights based on the error between predicted and actual outputs, allowing the network to effectively learn from its environment \citep{MurtaghMLP}. The architecture of an MLP, including the number of layers and nodes, along with the activation methods and learning techniques, significantly influences its performance. Balancing network complexity and the risk of overfitting is crucial in MLP design \citep{MurtaghMLP}. MLPs are versatile, capable of performing tasks like regression, mapping input vectors to values, and supervised classification, where input patterns are trained to produce specific output classifications \citep{MurtaghMLP}

