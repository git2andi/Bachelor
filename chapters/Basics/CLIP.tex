\section{Contrastive Language-Image Pre-training~--~CLIP}\label{CLIP}

\citeauthor{radfordCLIP} address the limitations of traditional computer vision models, which are restricted by their training on a fixed set of object categories and lack adaptability to new tasks or concepts. To overcome these limitations, they propose a novel method called Contrastive Language-Image Pre-training (CLIP), which is an ``efficient and scalable method of learning from natural language supervision''~\citep{radfordCLIP}. This process allows the model to learn a representation of the image that is grounded in natural language, enabling it to understand the content and context of the image.

Unlike traditional computer vision models that rely solely on annotated image datasets, CLIP leverages a large corpus of text and image pairs from the internet. It learns to associate images and their corresponding textual descriptions, allowing it to understand the relationship between visual and textual data. CLIP is built on a transformer-based architecture, which has proven highly effective for natural language processing tasks \citep{radfordCLIP}. It consists of two main components: an image encoder and a language encoder. The image encoder processes images using a modified version of ResNet50 \citep{heResnet} or as a second approach was build upon the Vision Transformer (ViT) \citep{dosovitskiyViT}, while the language encoder uses another modified transformer-based model to process textual descriptions \citep{vaswani2023attention}. By learning to associate images and text, CLIP acquires a generalized understanding of visual concepts and language semantics.

One of the remarkable aspects of CLIP is the ability for zero-shot capability. It can perform tasks without task-specific training. For example, given a natural language prompt, CLIP can recognize objects in images, generate captions, or perform classification tasks. Furthermore, CLIP has been shown to be very effective on a variety of tasks. It outperforms state-of-the-art models on the ImageNet classification task, and it achieves state-of-the-art results on the Visual Genome dataset for object detection and question answering \citep{radfordCLIP}.

However, there exist several limitations to CLIP\@. ``The performance of zero-shot CLIP is often just competitive with the supervised baseline of a linear classifier on ResNet-50 features''~\citep{radfordCLIP}.  This means that CLIP is not significantly better than a model that is trained on labeled data for the specific task at hand. In addition, the authors estimate that achieving SOTA performance across their evaluation suite would require significantly more computational resources, approximately a 1000-fold boost in computational power. Current hardware capabilities cannot accommodate such demands, emphasizing the requirement for advancements in hardware technology to effectively train zero-shot CLIP models.