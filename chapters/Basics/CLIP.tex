\section{Contrastive Language-Image Pre-training~--~CLIP}\label{CLIP}

\citeauthor{radfordCLIP} address the limitations of traditional computer vision models, which are restricted by their training on a fixed set of object categories and lack adaptability to new tasks or concepts. To overcome these limitations, they propose a novel method called Contrastive Language-Image Pre-training (CLIP), which is an ``efficient and scalable method of learning from natural language supervision''~\citep{radfordCLIP}. This process allows the model to learn a representation of the image that is grounded in natural language, enabling it to understand the content and context of the image.

Unlike traditional computer vision models that rely solely on annotated image datasets, CLIP leverages lots of text and image pairs from the internet. It learns to associate images and their corresponding textual descriptions, allowing it to understand the relationship between visual and textual data.

One of the remarkable aspects of CLIP is the ability for zero-shot capability. It can perform tasks without task-specific training \citep{radfordCLIP}. For example, given a natural language prompt, CLIP can recognize objects in images, generate captions, or perform classification tasks.

However, there exist several limitations to CLIP\@. ``The performance of zero-shot CLIP is often just competitive with the supervised baseline of a linear classifier on ResNet-50 features''~\citep{radfordCLIP}.  This means that CLIP is not significantly better than a model that is trained on labeled data for the specific task at hand.